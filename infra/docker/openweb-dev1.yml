# TermiteTowers Continuous Code Management Header TEMPLATE
# % ccm_modify_date: 2025-08-29 15:31:33 %
# % ccm_author: mpegg %
# % ccm_author_email: mpegg@hotmail.com %
# % ccm_repo: https://github.com/mpegg007/TermiteTowers.git %
# % ccm_branch: dev1 %
# % ccm_object_id: infra/docker/openweb-dev1.yml:0 %
# % ccm_commit_id: unknown %
# % ccm_commit_count: 0 %
# % ccm_commit_message: unknown %
# % ccm_commit_author: unknown %
# % ccm_commit_email: unknown %
# % ccm_commit_date: 1970-01-01 00:00:00 +0000 %
# % ccm_file_last_modified: 2025-08-29 15:31:33 %
# % ccm_file_name: openweb-dev1.yml %
# % ccm_file_type: text/plain %
# % ccm_file_encoding: us-ascii %
# % ccm_file_eol: CRLF %
# % ccm_path: infra/docker/openweb-dev1.yml %
# % ccm_blob_sha: 6df0ccf23886e9d578b32b83bc82a151f9fe68a6 %
# % ccm_exec: no %
# % ccm_size: 1915 %
# % ccm_tag:  %
# tt-ccm.header.end

services:
  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: openwebui-dev1
    user: "${UID_SVC:-2001}:${GID_TT_AI:-1006}"
    ports:
      - "0.0.0.0:3000:8080"
    environment:
      - OLLAMA_BASE_URL=http://host.docker.internal:11434
      - MODEL_PROVIDER=ollama
      - DEFAULT_MODEL=llama3
      - PYTHONUNBUFFERED=1
      - HF_HOME=/cache/huggingface
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - /mnt/ai_storage/openwebui/data:/app/backend/data
      - /mnt/ai_storage/models/huggingface:/cache/huggingface
      - /mnt/ai_storage/models/pip:/cache/pip
      - /mnt/ai_storage/models/torch:/cache/torch
    restart: unless-stopped
    logging:
      driver: json-file
      options:
        max-size: "50m"
        max-file: "5"
  # Note: If the container cannot reach Ollama via host.docker.internal on Linux,
  # you can switch to host networking as a fallback:
  # network_mode: host
  # and remove the ports/extra_hosts section. Then access at http://localhost:8080
